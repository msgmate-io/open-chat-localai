services:
  api:
    image: quay.io/go-skynet/local-ai:master-ffmpeg-core
    build:
      context: .
      dockerfile: Dockerfile
      args:
      - IMAGE_TYPE=core
      - BASE_IMAGE=ubuntu:22.04
    ports:
      - 8080:8080
    env_file:
      - .env
    environment:
      - MODELS_PATH=/models
      - LOCALAI_P2P=true
      - LLAMACPP_PARALLEL=2
      - LOCALAI_PARALLEL_REQUESTS=true
    network_mode: "host"
    volumes:
      - ./models:/models:cached
    command:
    - meta-llama-3.1-8b-instruct
    - whisper-1
    - whisper-large-q5_0
    - whisper-small-q5_1
    - whisper-medium-q5_0
    - phi-3.1-mini-4k-instruct
    - tess-v2.5-phi-3-medium-128k-14b
    - smollm-1.7b-instruct
    - emo-2b
    - llama3-70b-instruct 
