services:
  api-worker:
    image: quay.io/go-skynet/local-ai:master-ffmpeg-core
    build:
      context: .
      dockerfile: Dockerfile
      args:
      - IMAGE_TYPE=core
      - BASE_IMAGE=ubuntu:22.04
    ports:
      - 8081:8080
    env_file:
      - .env
    environment:
      - TOKEN=b3RwOgogIGRodDoKICAgIGludGVydmFsOiA5MDAKICAgIGtleTogOU1nbjFibnQ0bGM0cmxKbk92cWhsQXhlcUdqOTgxQkozMDRHM1djTDJ4TwogICAgbGVuZ3RoOiA0MwogIGNyeXB0bzoKICAgIGludGVydmFsOiA5MDAKICAgIGtleTogM29DYzJ0aFFqelFDN2k1UzZ2NjNXelY4engwZ0pxdkg5akl1TnY1Q0UyVQogICAgbGVuZ3RoOiA0Mwpyb29tOiAzam84VDhDRU03RFhtc1BuckJuN1R2U1drb3EyZHlURUF4T1pBS00zUEZPCnJlbmRlenZvdXM6IFpLSlQ2VzVJV0p3a1RHdzVhd3A2dDVQQ1YzazZSdlhPcEFGcjdsdHVPNUgKbWRuczogRFpkUHlWSXBDU1NhZmVIdE5PaGJiUDNacm5kNE1LcHNVT3JSUEV5NHdQWAptYXhfbWVzc2FnZV9zaXplOiAyMDk3MTUyMAo=
      - MODELS_PATH=/models
      - P2P_LLAMA_CPP_RPC_ENABLED=true
      - LOCALAI_NODE_TYPE=worker
    network_mode: "host"
    volumes:
      - ./models:/models:cached
    command:
    - worker
    - p2p-llama-cpp-rpc
